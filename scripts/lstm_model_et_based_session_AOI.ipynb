{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNQuhAp83--q"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "ebz1fKj03MSq",
    "outputId": "fd182e98-699e-4027-c64c-edc78b882286",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:                 2 / 2, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:                 2 / 2, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:                14 / 14, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:                74 / 74, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:               191 / 191, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:               191 / 191, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:               210 / 210, 100%\n",
      "Elapsed time:         0.0s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "'''\n",
    "Initial setup\n",
    "'''\n",
    "project_id = ''\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir(\"/content/drive\")\n",
    "\n",
    "    base_drive = 'Shared drives/Columbia Eyetracking Data/'\n",
    "    data_dir = 'data/'\n",
    "    os.chdir(base_drive)\n",
    "elif not IN_COLAB:\n",
    "    root_data_dir = '/home/jupyter/nsf-neurogesture/data_drive_sync'\n",
    "    \n",
    "    # 1. install command: curl https://rclone.org/install.sh | sudo bash\n",
    "    # 2. follow instructions to set up credentials, use remote login in last step: https://rclone.org/drive/\n",
    "    # !rclone sync nsf_neurogesture:data {root_data_dir} -P\n",
    "    !rclone copy nsf_neurogesture:data {root_data_dir} -P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_run = '10-5'\n",
    "curr_dir = root_data_dir+ '/processed_data/'+curr_run + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nuBVtxHn3-nP",
    "outputId": "91c637b8-c3f5-4f8d-fded-ce8c10f60da3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow_core._api.v2.version' from '/opt/conda/lib/python3.7/site-packages/tensorflow_core/_api/v2/version/__init__.py'>\n",
      "Num GPUs Available:  0\n",
      "Not using GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow-gpu\n",
    "#!pip install -U scikit-learn\n",
    "#!kill -9 -1\n",
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Reshape, BatchNormalization, LeakyReLU, concatenate, Input, LSTM, Masking, Dense, TimeDistributed, Bidirectional, Activation, Embedding, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "print(tf.version)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "    print(tf.DeviceSpec(device_type=\"GPU\", device_index=34343))\n",
    "    gpus = True\n",
    "else:\n",
    "    gpus = False\n",
    "    print(\"Not using GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "colab_type": "code",
    "id": "1FoCwUSKgv8G",
    "outputId": "1f4a2074-aa19-4eeb-cf72-c55758c945df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions:  ('vid_type-topic-pp-block', 'block', 'feature')\n",
      "The attributes:  Coordinates:\n",
      "  * vid_type-topic-pp-block  (vid_type-topic-pp-block) <U24 'full-tarmac-a2-1' ... 'single-perspective-z2-19'\n",
      "  * block                    (block) int64 1 2 3 4 5 6 7 ... 46 47 48 49 50 51\n",
      "  * feature                  (feature) <U26 'fixation_start' ... 'blink_start_diff'\n",
      "The features:  <xarray.DataArray 'feature' (feature: 19)>\n",
      "array(['fixation_start', 'fixation_duration', 'fixation_xAvg', 'fixation_yAvg',\n",
      "       'fixation_pupilAvg', 'fixation_start_diff', 'fixation_xAvg_diff',\n",
      "       'fixation_yAvg_diff', 'fixation_aoi', 'saccades_start',\n",
      "       'saccades_duration', 'saccades_xEnd_minus_xStart',\n",
      "       'saccades_yEnd_minus_yStart', 'saccades_ampDeg', 'saccades_vPeak',\n",
      "       'saccades_start_diff', 'blink_start', 'blink_duration',\n",
      "       'blink_start_diff'], dtype='<U26')\n",
      "Coordinates:\n",
      "    vid_type-topic-pp-block  <U24 'full-bicycle-g3-1'\n",
      "  * feature                  (feature) <U26 'fixation_start' ... 'blink_start_diff'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/histograms.py:840: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var_to_exclude = ['fixation_pupilAvg']\n",
    "loss = 'mean_squared_error'\n",
    "early_stopping_loss = loss\n",
    "final_activation='relu'\n",
    "batch_size = 32\n",
    "epochs = 5000\n",
    "optimizer = tf.optimizers.Adam(lr=0.0001)\n",
    "#optimizer = tf.train.AdamOptimizer(0.001)\n",
    "output_scaling = False\n",
    "output_limit = False\n",
    "filter_by_mean = False\n",
    "\n",
    "dataset = pd.read_pickle(curr_dir+'processed_tensor.pkl')\n",
    "labels = np.load(curr_dir+'labels.npy')\n",
    "scaler = StandardScaler() # use the same scaler for all X\n",
    "lb = LabelEncoder()\n",
    "#imp_mean = SimpleImputer(missing_values=np.float(-999), strategy='mean')\n",
    "if output_scaling:\n",
    "  output_scaler = MinMaxScaler()\n",
    "  labels = output_scaler.fit_transform(labels.reshape(-1, 1))\n",
    "  labels = labels.reshape(-1)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.hist(labels)\n",
    "plt.ylabel('Freq');\n",
    "\n",
    "print('The dimensions: ', dataset.dims)\n",
    "print('The attributes: ', dataset.coords)\n",
    "print('The features: ', dataset['feature'].values)\n",
    "#print('One sample: ', dataset.loc['full-bicycle-g3-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9w6s6rU1ty5L"
   },
   "outputs": [],
   "source": [
    "def filter_training(cond):\n",
    "  accuracy_csv = pd.read_csv('../accuracy_measures.csv')\n",
    "  accuracy_csv = accuracy_csv.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "  keys_arr = list(dataset.keys())\n",
    "  keys_arr = list(map(lambda x: x.split('-') , keys_arr))\n",
    "  if cond == 'overall_2_bins':\n",
    "    average_dict = {\n",
    "      'bicycle': 'BQAVG',\n",
    "        'perspective': 'PQAVG',\n",
    "        'tarmac': 'TQAVG'\n",
    "    }\n",
    "    mid_point = np.mean(pd.concat([accuracy_csv['TQAVG'],accuracy_csv['PQAVG'],accuracy_csv['BQAVG']]))\n",
    "    print('splitting each sequence based on the mean of overall accuracy. each participant will belong to a higher or lower category depending on their overall accuracy > or < : ', mid_point)\n",
    "    output = []\n",
    "    for key in keys_arr:\n",
    "      output.append(float(accuracy_csv[(accuracy_csv.ParticipantID == key[2]) & (accuracy_csv.Condition == key[0])][average_dict[key[1]]]))\n",
    "    greater_than = [i for i,v in enumerate(output) if v > mid_point]\n",
    "    bins_output = []\n",
    "    for num in range(len(output)):\n",
    "      if num in greater_than:\n",
    "        bins_output.append(2)  # greater than mid point\n",
    "      else:\n",
    "        bins_output.append(1)\n",
    "  return bins_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0CQNTqec3Ua"
   },
   "outputs": [],
   "source": [
    "def data_prep(dataset, labels, feature_list, cat_idx, num_idx):\n",
    "    if output_limit:\n",
    "      #labels = np.where(labels > np.float32(-1000), labels, np.nan)\n",
    "      #labels = np.where(labels < np.float32(1000), labels, np.nan)\n",
    "      labels = labels + np.absolute(np.min(labels))\n",
    "    if filter_by_mean:\n",
    "      bins_return = filter_training('overall_2_bins')  # split data by overall average\n",
    "      greater_than = [i for i,v in enumerate(bins_return) if v == 2]\n",
    "      lesser_than = [i for i,v in enumerate(bins_return) if v == 1]\n",
    "      labels = labels[greater_than]\n",
    "      dataset = dataset.iloc[greater_than]\n",
    "    # mask labels that are nan\n",
    "    index = ~np.isnan(labels)\n",
    "    labels = labels[index]\n",
    "    mask = np.ones(len(dataset), dtype=bool)\n",
    "    mask = index\n",
    "    dataset = dataset[mask,]\n",
    "    %matplotlib inline\n",
    "    plt.hist(labels)\n",
    "    plt.ylabel('Freq');\n",
    "    plt.show()\n",
    "    dataset_keys = dataset['vid_type-topic-pp-block'] # for lookup later\n",
    "    dataset = dataset.values\n",
    "    #encode AOI, will need to be modified if more categoricals are added later\n",
    "    lb.fit(['i','t','p','nan'])\n",
    "    print('aoi labels: ', lb.classes_)\n",
    "    for pp in range(dataset.shape[0]):\n",
    "        thiscol = dataset[pp,:,cat_idx[0]].astype(str)\n",
    "        dataset[pp,:,cat_idx[0]] = lb.transform(thiscol)\n",
    "    # ensure all data is float\n",
    "    values = dataset.astype('float32')\n",
    "\n",
    "    # isolate numerical features (refernce: https://stackoverflow.com/questions/52627739/how-to-merge-numerical-and-embedding-sequential-models-to-treat-categories-in-rn)\n",
    "    num_cats = len(cat_idx)  # number of categorical features\n",
    "    n_steps = values.shape[1]  # number of timesteps in each sample\n",
    "    n_numerical_feats = len(num_idx)  # number of numerical features in each sample\n",
    "    cat_embd_dim = [50]  # embedding dimension for each categorical feature, for now just AOI\n",
    "    numerical_input = Input(shape=(n_steps, n_numerical_feats), name='numeric_input')\n",
    "    cat_inputs = []\n",
    "    cat_size = []\n",
    "    for i in range(num_cats):\n",
    "        cat_inputs.append(Input(shape=(n_steps,), name='cat' + str(i + 1) + '_input'))\n",
    "        cat_size.append(len(lb.classes_))  # number of categories in each categorical feature\n",
    "\n",
    "    cat_embedded = []\n",
    "    for i in range(num_cats):\n",
    "        embed = Embedding(cat_size[i], cat_embd_dim[i])(cat_inputs[i])\n",
    "        cat_embedded.append(embed)\n",
    "        pass\n",
    "        \n",
    "    # impute\n",
    "    si =  SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    for i in range(len(num_idx)):\n",
    "        feat_mean = np.nanmean(values[:,:,num_idx[i]])\n",
    "        for pp in range(values.shape[0]):\n",
    "            imputed = si.fit_transform(values[pp,:,num_idx[i]].reshape(-1,1))\n",
    "            if imputed.shape[1] == 0:\n",
    "                values[pp,:,num_idx[i]] = feat_mean # fill with feature mean\n",
    "            else:    \n",
    "                values[pp,:,num_idx[i]] = np.squeeze(imputed)\n",
    "    X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(values,\n",
    "                                                        labels,range(len(dataset)),\n",
    "                                                        test_size=0.20)\n",
    "    return (X_train, X_test, y_train, y_test, numerical_input, cat_embedded, \n",
    "            cat_inputs, dataset_keys[indices_train], dataset_keys[indices_test])\n",
    "\n",
    "def scale_x(x):\n",
    "    # reshape values\n",
    "    nsamples, nx, num_features = x.shape\n",
    "    values = np.reshape(x, (-1, num_features))\n",
    "    values = np.where(values==np.nan, 0, values)\n",
    "    #values = imp_mean.fit_transform(values)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    scaled = np.reshape(scaled, (nsamples, nx, num_features))\n",
    "    return scaled\n",
    "\n",
    "def scale_x_test(x):\n",
    "    # reshape values\n",
    "    nsamples, nx, num_features = x.shape\n",
    "    values = np.reshape(x, (-1, num_features))\n",
    "    values = np.where(values==np.nan, 0, values)\n",
    "    #values = imp_mean.transform(values)\n",
    "    scaled = scaler.transform(values)\n",
    "    scaled = np.reshape(scaled, (nsamples, nx, num_features))\n",
    "    return scaled\n",
    "\n",
    "# root mean squared error (rmse) for regression\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# coefficient of determination (R^2) for regression\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  backend.sum(backend.square(y_true - y_pred))\n",
    "    SS_tot = backend.sum(backend.square(y_true - backend.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + backend.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Architecture with AOI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hsVe7J-FvC2"
   },
   "outputs": [],
   "source": [
    "def vanilla_lstm(run_no, X_train, y_train, X_test, y_test, numerical_input, cat_embedded, cat_inputs, num_idx, cat_idx, n_steps):\n",
    "    # define the checkpoint\n",
    "    filepath = curr_dir+str(run_no)+'/'+'model.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    earlystopping = EarlyStopping(monitor=early_stopping_loss, patience=500, verbose=1, mode='auto')\n",
    "    log_loc = curr_dir+str(run_no)+'/'+\"./logs\"\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_loc, histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_grads=True,\n",
    "                         batch_size=batch_size,\n",
    "                         write_images=True,\n",
    "                         update_freq = 'epoch')\n",
    "    callbacks_list = [checkpoint, earlystopping, tensorboard_callback]\n",
    "    cat_merged = cat_embedded[0]  # can be another concatenate if more than one categorical\n",
    "    merged = concatenate([numerical_input, cat_merged])\n",
    "    x = (Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))(merged)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = (Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = (Dense(units=1, activation=final_activation))(x)\n",
    "    model = Model(inputs=[numerical_input] + cat_inputs, outputs=x)\n",
    "    model.summary()\n",
    "    \n",
    "    X_tr_numerical = scale_x(X_train[:, :, num_idx])\n",
    "    X_te_numerical = scale_x_test(X_test[:, :, num_idx])\n",
    "\n",
    "    \n",
    "    X_tr_cat1 = np.squeeze(X_train[:, :, cat_idx], axis=2)\n",
    "    X_te_cat1 = np.squeeze(X_test[:, :, cat_idx], axis=2)\n",
    "    model.compile(\n",
    "          optimizer=optimizer,\n",
    "          loss=loss,\n",
    "          metrics=[loss, rmse, r_square]\n",
    "      )\n",
    "    # inline tensorboard (more limited)\n",
    "    #%tensorboard --logdir logs --port=6006\n",
    "    result = model.fit({\n",
    "          'numeric_input': X_tr_numerical,\n",
    "          'cat1_input': X_tr_cat1},\n",
    "          y_train, \n",
    "          epochs=epochs, batch_size=batch_size, validation_split=0.15, callbacks=callbacks_list)\n",
    "    model.evaluate({'numeric_input': X_te_numerical,\n",
    "               'cat1_input': X_te_cat1},\n",
    "               y_test)\n",
    "    # get predictions\n",
    "    y_pred = model.predict({'numeric_input': X_te_numerical,\n",
    "               'cat1_input': X_te_cat1})\n",
    "    with open(curr_dir+str(run_no)+'/'+'training_history.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(result.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results each Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0ow5uiMvJNr"
   },
   "outputs": [],
   "source": [
    "def load_existing_model(load_model_path, result_path, X_train, y_train, X_test, y_test, numerical_input, cat_embedded, cat_inputs,\n",
    "                        num_idx, cat_idx, n_steps):\n",
    "    X_tr_numerical = scale_x(X_train[:, :, num_idx])\n",
    "    X_te_numerical = scale_x_test(X_test[:, :, num_idx])\n",
    "    # extract categorical features: you can use a for loop to this as well.\n",
    "    # note that we reshape categorical features to make them consistent with the updated solution\n",
    "    X_tr_cat1 = np.squeeze(X_train[:, :, cat_idx], axis=2)\n",
    "    X_te_cat1 = np.squeeze(X_test[:, :, cat_idx], axis=2)\n",
    "\n",
    "    model = tf.keras.models.load_model(load_model_path, custom_objects={'rmse': rmse, 'r_square': r_square})\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[loss, rmse, r_square]\n",
    "    )\n",
    "    model.evaluate({'numeric_input': X_te_numerical,\n",
    "                                 'cat1_input': X_te_cat1},\n",
    "                                y_test)\n",
    "    # get predictions\n",
    "    y_pred = model.predict({'numeric_input': X_te_numerical,\n",
    "                            'cat1_input': X_te_cat1})\n",
    "    if output_scaling:\n",
    "      y_pred = output_scaler.inverse_transform(y_pred)\n",
    "      y_test = output_scaler.inverse_transform(y_test)\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Plot learning curves including R^2 and RMSE\n",
    "    # -----------------------------------------------------------------------------\n",
    "    if os.path.exists(result_path):\n",
    "      result = pickle.load( open(result_path, \"rb\" ) )\n",
    "      # plot training curve for R^2 (beware of scale, starts very low negative)\n",
    "      plt.plot(result['val_r_square'])\n",
    "      plt.plot(result['r_square'])\n",
    "      plt.title('model R^2')\n",
    "      plt.ylabel('R^2')\n",
    "      plt.xlabel('epoch')\n",
    "      plt.legend(['train', 'test'], loc='upper left')\n",
    "      plt.show()\n",
    "\n",
    "      # plot training curve for rmse\n",
    "      plt.plot(result['rmse'])\n",
    "      plt.plot(result['val_rmse'])\n",
    "      plt.title('rmse')\n",
    "      plt.ylabel('rmse')\n",
    "      plt.xlabel('epoch')\n",
    "      plt.legend(['train', 'test'], loc='upper left')\n",
    "      plt.show()\n",
    "    \n",
    "    # print the linear regression and display datapoints\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(y_test.reshape(-1, 1), y_pred)\n",
    "    y_fit = regressor.predict(y_pred)\n",
    "\n",
    "    reg_intercept = round(regressor.intercept_[0], 4)\n",
    "    reg_coef = round(regressor.coef_.flatten()[0], 4)\n",
    "    reg_label = \"y = \" + str(reg_intercept) + \"*x +\" + str(reg_coef)\n",
    "\n",
    "    plt.scatter(y_test, y_pred, color='blue', label='data')\n",
    "    plt.plot(y_pred, y_fit, color='red', linewidth=2, label='Linear regression\\n' + reg_label)\n",
    "    #plt.ylim(1913.95, 1913.96)\n",
    "    plt.title('Linear Regression')\n",
    "    plt.legend()\n",
    "    plt.xlabel('observed')\n",
    "    plt.ylabel('predicted')\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 10 Times and Plot Predicted Test Sample Pupil Diameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "colab_type": "code",
    "id": "wORQ79ke9DXM",
    "outputId": "7e9de0a6-80fa-4082-91ab-ad703360b7ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_runs(run_num=10):\n",
    "    feature_list = list(dataset['feature'].values)\n",
    "    categorical_var_list = ['fixation_aoi']\n",
    "    mask_categorical_outcome = list(categorical_var_list)\n",
    "    mask_categorical_outcome.extend(var_to_exclude)\n",
    "    cat_idx = [feature_list.index(cat) for cat in categorical_var_list]\n",
    "    num_vars_list = [i for i in feature_list if i not in mask_categorical_outcome]\n",
    "    num_idx = [feature_list.index(num) for num in num_vars_list]\n",
    "    for i in range(run_num):\n",
    "        if not os.path.exists(str(i)):\n",
    "            os.makedirs(str(i))\n",
    "        X_train, X_test, y_train, y_test, numerical_input, cat_embedded, cat_inputs, train_keys, test_keys= data_prep(dataset, labels, feature_list, cat_idx, num_idx)\n",
    "        if not os.path.exists(str(i)+'/'+'model.h5'):\n",
    "            with open(str(i)+'/'+'saved_data.pkl', 'wb') as f:\n",
    "                pickle.dump([X_train, y_train, X_test, y_test, num_idx, cat_idx, train_keys, test_keys], f)\n",
    "            vanilla_lstm(i, X_train, y_train, X_test, y_test, numerical_input, cat_embedded, cat_inputs, num_idx, cat_idx, n_steps=dataset.shape[1]) \n",
    "        else:\n",
    "          if os.path.exists('saved_data.pkl'):\n",
    "            with open(str(i)+'/'+'saved_data.pkl', 'rb') as f:\n",
    "              X_train, y_train, X_test, y_test, num_idx, cat_idx, train_keys, test_keys = pickle.load(f)\n",
    "              # to test for whether model is doing anything\n",
    "              #np.random.shuffle(y_test)\n",
    "        y_pred = load_existing_model(str(i)+'/'+'model.h5',str(i)+'/'+'training_history.pkl', X_train, y_train, X_test, y_test, numerical_input, cat_embedded, cat_inputs, num_idx, cat_idx, n_steps=dataset.shape[1])\n",
    "        print('done')\n",
    "\n",
    "model_runs(run_num=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "lstm_model_et_based_session_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
