{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse EyeLink Data and Prep for LSTM Model Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Cloud or Colab Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:                 2 / 2, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:                 2 / 2, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:                15 / 15, 100%\n",
      "Elapsed time:         0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1GTransferred:   \t         0 / 0 Bytes, -, 0 Bytes/s, ETA -\n",
      "Checks:               198 / 198, 100%\n",
      "Elapsed time:         0.0s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "'''\n",
    "Initial setup\n",
    "'''\n",
    "project_id = ''\n",
    "bucket_name = 'nsf-neurogesture'\n",
    "import_folder = bucket_name+'/all_eyelink_data.npy'\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "elif not IN_COLAB:\n",
    "    root_data_dir = '/home/jupyter/nsf-neurogesture/data_drive_sync'\n",
    "    \n",
    "    # 1. install command: curl https://rclone.org/install.sh | sudo bash\n",
    "    # 2. follow instructions to set up credentials, use remote login in last step: https://rclone.org/drive/\n",
    "    !rclone copy nsf_neurogesture:data {root_data_dir} -P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParseEyeLinkAsc.py\n",
    "# - Reads in .asc data files from EyeLink and produces pandas dataframes for further analysis\n",
    "#\n",
    "# Created 7/31/18-8/15/18 by DJ.\n",
    "\n",
    "\n",
    "def ParseEyeLinkAsc(elFilename):\n",
    "    # dfTrial,dfMsg,dfFix,dfSacc,dfBlink,dfSamples = ParseEyeLinkAsc(elFilename)\n",
    "    # -Reads in data files from EyeLink .asc file and produces readable dataframes for further analysis.\n",
    "    #\n",
    "    # INPUTS:\n",
    "    # -elFilename is a string indicating an EyeLink data file from an AX-CPT task in the current path.\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # -dfTrial contains information about trials\n",
    "    # -dfMsg contains information about messages (usually sent from stimulus software)\n",
    "    # -dfFix contains information about fixations\n",
    "    # -dfSacc contains information about saccades\n",
    "    # -dfBlink contains information about blinks\n",
    "    # -dfSamples contains information about individual samples\n",
    "    #\n",
    "    # Created 7/31/18-8/15/18 by DJ.\n",
    "    \n",
    "    # Import packages\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "\n",
    "    # ===== READ IN FILES ===== #\n",
    "    # Read in EyeLink file\n",
    "    #print('Reading in EyeLink file %s...'%elFilename)\n",
    "    t = time.time()\n",
    "    f = open(elFilename,'r')\n",
    "    fileTxt0 = f.read().split(\"\\n\") # split into lines (runs)\n",
    "   # fileTxt0 = list(filter(None, fileTxt0)) #  remove emptys\n",
    "    fileTxt0 = np.array(fileTxt0) # concert to np array for simpler indexing\n",
    "    f.close()\n",
    "    #print('Done! Took %f seconds.'%(time.time()-t))\n",
    "\n",
    "    # Separate lines into samples and messages\n",
    "    #print('Sorting lines...')\n",
    "    nLines = len(fileTxt0)\n",
    "    lineType = np.array(['OTHER']*nLines,dtype='object')\n",
    "    iStartRec = None\n",
    "    t = time.time()\n",
    "    for iLine in range(nLines):\n",
    "        if len(fileTxt0[iLine])<3:\n",
    "            lineType[iLine] = 'EMPTY'\n",
    "        elif fileTxt0[iLine].startswith('*') or fileTxt0[iLine].startswith('>>>>>'):\n",
    "            lineType[iLine] = 'COMMENT'\n",
    "        elif fileTxt0[iLine].split()[0][0].isdigit() or fileTxt0[iLine].split()[0].startswith('-'):\n",
    "            lineType[iLine] = 'SAMPLE'\n",
    "        else:\n",
    "            lineType[iLine] = fileTxt0[iLine].split()[0]\n",
    "        if '!CAL' in fileTxt0[iLine]: # TODO: Find more general way of determining if recording has started\n",
    "            iStartRec = iLine+1\n",
    "    #print('Done! Took %f seconds.'%(time.time()-t))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ===== PARSE EYELINK FILE ===== #\n",
    "    t = time.time()\n",
    "    # Trials\n",
    "    #print('Parsing trial markers...')\n",
    "    iNotStart = np.nonzero(lineType!='START')[0]\n",
    "    dfTrialStart = pd.read_csv(elFilename,skiprows=iNotStart,header=None,delim_whitespace=True,usecols=[1])\n",
    "    dfTrialStart.columns = ['tStart']\n",
    "    iNotEnd = np.nonzero(lineType!='END')[0]\n",
    "    dfTrialEnd = pd.read_csv(elFilename,skiprows=iNotEnd,header=None,delim_whitespace=True,usecols=[1,5,6])\n",
    "    dfTrialEnd.columns = ['tEnd','xRes','yRes']\n",
    "    # combine trial info\n",
    "    dfTrial = pd.concat([dfTrialStart,dfTrialEnd],axis=1)\n",
    "    nTrials = dfTrial.shape[0]\n",
    "    #print('%d trials found.'%nTrials)\n",
    "\n",
    "    # Import Messages\n",
    "    #print('Parsing stimulus messages...')\n",
    "    t = time.time()\n",
    "    iMsg = np.nonzero(lineType=='MSG')[0]\n",
    "    # set up\n",
    "    tMsg = []\n",
    "    txtMsg = []\n",
    "    t = time.time()\n",
    "    for i in range(len(iMsg)):\n",
    "        # separate MSG prefix and timestamp from rest of message\n",
    "        info = fileTxt0[iMsg[i]].split()\n",
    "        # extract info\n",
    "        tMsg.append(int(info[1]))\n",
    "        txtMsg.append(' '.join(info[2:]))\n",
    "    # Convert dict to dataframe\n",
    "    dfMsg = pd.DataFrame({'time':tMsg, 'text':txtMsg})\n",
    "    #print('Done! Took %f seconds.'%(time.time()-t))\n",
    "    \n",
    "    # Import Fixations\n",
    "    #print('Parsing fixations...')\n",
    "    t = time.time()\n",
    "    iNotEfix = np.nonzero(lineType!='EFIX')[0]\n",
    "    dfFix = pd.read_csv(elFilename,skiprows=iNotEfix,header=None,delim_whitespace=True,usecols=range(1,8))\n",
    "    dfFix.columns = ['eye','tStart','tEnd','duration','xAvg','yAvg','pupilAvg']\n",
    "    nFix = dfFix.shape[0]\n",
    "    #print('Done! Took %f seconds.'%(time.time()-t))\n",
    "\n",
    "    # Saccades\n",
    "    #print('Parsing saccades...')\n",
    "    t = time.time()\n",
    "    iNotEsacc = np.nonzero(lineType!='ESACC')[0]\n",
    "    dfSacc = pd.read_csv(elFilename,skiprows=iNotEsacc,header=None,delim_whitespace=True,usecols=range(1,11))\n",
    "    dfSacc.columns = ['eye','tStart','tEnd','duration','xStart','yStart','xEnd','yEnd','ampDeg','vPeak']\n",
    "    #print('Done! Took %f seconds.'%(time.time()-t))\n",
    "    \n",
    "    # Blinks\n",
    "    #print('Parsing blinks...')\n",
    "    iNotEblink = np.nonzero(lineType!='EBLINK')[0]\n",
    "    dfBlink = pd.read_csv(elFilename,skiprows=iNotEblink,header=None,delim_whitespace=True,usecols=range(1,5))\n",
    "    dfBlink.columns = ['eye','tStart','tEnd','duration']\n",
    "    #print('Done! Took %f seconds.'%(time.time()-t))\n",
    "    \n",
    "    # Import samples\n",
    "    #print('Parsing samples...')\n",
    "    t = time.time()\n",
    "    if iStartRec:\n",
    "        iNotSample = np.nonzero( np.logical_or(lineType!='SAMPLE', np.arange(nLines)<iStartRec))[0]\n",
    "        dfSamples = pd.read_csv(elFilename, skiprows=iNotSample,header=None,delim_whitespace=True,usecols=range(0,4))\n",
    "    else:\n",
    "        iNotSample = np.nonzero(np.logical_or(lineType != 'SAMPLE', False))[0]\n",
    "        dfSamples = pd.read_csv(elFilename, skiprows=iNotSample, header=None, delim_whitespace=True,\n",
    "                                usecols=range(0, 4))\n",
    "    dfSamples.columns = ['tSample', 'LX', 'LY', 'LPupil']\n",
    "    # Convert values to numbers\n",
    "    dfSamples['LX'] = pd.to_numeric(dfSamples['LX'],errors='coerce')\n",
    "    dfSamples['LY'] = pd.to_numeric(dfSamples['LY'],errors='coerce')\n",
    "    #dfSamples['RX'] = pd.to_numeric(dfSamples['RX'],errors='coerce')\n",
    "    #dfSamples['RY'] = pd.to_numeric(dfSamples['RY'],errors='coerce')\n",
    "    #print('Done! Took %.1f seconds.'%(time.time()-t))\n",
    "\n",
    "    #make time relative to trial start\n",
    "    dfFix['tStart'] = dfFix['tStart'] - dfTrialStart['tStart'][0]\n",
    "    dfFix['tEnd'] = dfFix['tEnd'] - dfTrialStart['tStart'][0]\n",
    "    dfSacc['tStart'] = dfSacc['tStart'] - dfTrialStart['tStart'][0]\n",
    "    dfSacc['tEnd'] = dfSacc['tEnd'] - dfTrialStart['tStart'][0]\n",
    "    dfBlink['tStart'] = dfBlink['tStart'] - dfTrialStart['tStart'][0]\n",
    "    dfBlink['tEnd'] = dfBlink['tEnd'] - dfTrialStart['tStart'][0]\n",
    "    dfSamples['tSample'] = dfSamples['tSample'] - dfTrialStart['tStart'][0]\n",
    "    # Return new compilation dataframe\n",
    "    return dfTrial,dfMsg,dfFix,dfSacc,dfBlink,dfSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project-specific Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import math, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "sys.path.insert(0, 'scripts')\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from functools import reduce\n",
    "import xarray as xr\n",
    "import pickle\n",
    "\n",
    "def load_questions(vid_labels, filter_out=True):\n",
    "    '''\n",
    "    Load relevant data containing questions mentioned in videos.\n",
    "    :return: dictionary where keys = vid_labels below, and values are sub-dict with question\n",
    "            number and tuples of start and end times of video regions containing answer content.\n",
    "            Gesture answer content is contained within 'question_on' and 'question_off' so just\n",
    "            need to read these.\n",
    "    '''\n",
    "    type_slide_data = []\n",
    "    vid_dict = {}\n",
    "    for v in vid_labels:\n",
    "        vid_dict[v] = None\n",
    "        type_slide_data.append(root_data_dir+'/raw_data/slides/' + v + '_sqg_v2.csv')\n",
    "    vid_counter = 0\n",
    "    for csvtype in type_slide_data:\n",
    "        type_csv = pd.read_csv(csvtype)\n",
    "        # sort by questions and fixation times\n",
    "        type_csv = type_csv.sort_values(by=['questionno', 'question_on'])\n",
    "        qdict = collections.defaultdict(list)\n",
    "        # go through each question and create a dictionary\n",
    "        for index, row in type_csv.iterrows():\n",
    "            if not math.isnan(row['question_on']):\n",
    "                q = int(row['questionno'])\n",
    "                # filtering out questions\n",
    "                if filter_out:\n",
    "                    if 'filter_out' in type_csv.columns and row['filter_out'] == 0:\n",
    "                        qdict[q].append((row['question_on'], row['question_off']))\n",
    "                    elif 'filter_out' not in type_csv.columns:\n",
    "                        qdict[q].append((row['question_on'], row['question_off']))\n",
    "                else:\n",
    "                    qdict[q].append((row['question_on'], row['question_off']))\n",
    "        vid_dict[vid_labels[vid_counter]] = qdict\n",
    "        vid_counter += 1\n",
    "    return vid_dict\n",
    "\n",
    "\n",
    "def loadFiles(input_directory):\n",
    "    '''\n",
    "        Loads all raw asc files.\n",
    "        :param: Directory of asc files:\n",
    "        :return: List of dataframes, specific to each vid type, condition and participant\n",
    "    '''\n",
    "\n",
    "    vid_types = ['full', 'dual', 'single']\n",
    "    conds = {'bic': 'bicycle', 'per': 'perspective', 'tar': 'tarmac'}\n",
    "    output = {}\n",
    "    # this is the output order for reference\n",
    "    for vid_type in vid_types:\n",
    "        for cond in conds:\n",
    "            print(input_directory + '/' + vid_type + '/' + cond + '/*')\n",
    "            fixation_files = glob.glob(input_directory + '/' + vid_type + '/' + cond + '/*')\n",
    "            for file in fixation_files:\n",
    "                dfTrial, dfMsg, dfFix, dfSacc, dfBlink, dfSamples = ParseEyeLinkAsc(file)\n",
    "                output[(vid_type, conds[cond], file.split('.asc')[0][-2:])] = (dfTrial, dfMsg, dfFix, dfSacc, dfBlink,\n",
    "                                                                               dfSamples)\n",
    "                print('read asc: ', vid_type, conds[cond], file.split('.asc')[0][-2:])\n",
    "    return output\n",
    "\n",
    "\n",
    "def merge_fixations_assessments():\n",
    "    '''\n",
    "    Loads dictionary of question and answer content.\n",
    "    :return: output dataframe\n",
    "    '''\n",
    "    df = pd.DataFrame(columns=['vid_type', 'cond', 'pid', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', ' q7'])\n",
    "    score_files = glob.glob(root_data_dir+'/raw_data/assessments/*.csv')\n",
    "    row_num = 0\n",
    "    for file in score_files:\n",
    "        print('reading score file: ', file)\n",
    "        if os.name == 'nt':\n",
    "            vid_type = file.split('_')[-2].split('\\\\')[1]\n",
    "        else:\n",
    "            vid_type = file.split('_')[-2].split('/')[2]\n",
    "        cond = file.split('_')[-1].split('.')[0]\n",
    "        incsv = pd.read_csv(file)\n",
    "        for _, row in incsv.iterrows():\n",
    "            if not pd.isnull(row['ParticipantID']):\n",
    "                df.loc[row_num] = [vid_type, cond, row['ParticipantID'], row['Q1'], row['Q2'], row['Q3'], row['Q4'],\n",
    "                                   row['Q5'], row['Q6'], row['Q7']]\n",
    "                row_num += 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_aois():\n",
    "    aois_df = pd.read_csv(root_data_dir+'/raw_data/slides/eye_tracking_bounds.csv', dtype={'start_s': 'float64',\n",
    "                                                                               'end_s': 'float64',\n",
    "                                                                               'x_start': 'float64',\n",
    "                                                                               'x_end': 'float64',\n",
    "                                                                               'y_start': 'float64',\n",
    "                                                                               'y_end': 'float64'})\n",
    "    aois_df['start_s'] = aois_df['start_s']*1000\n",
    "    aois_df['end_s'] = aois_df['end_s'] * 1000\n",
    "    return aois_df.to_records()\n",
    "\n",
    "\n",
    "def aoi_lookup(aois_df, lookup_time, lookup_x, lookup_y, vid_type, cond):\n",
    "    '''\n",
    "    :param aois_df: reference to dataframe containing aoi information\n",
    "    :param lookup_time: typically fixation or saccade times, in ms\n",
    "    :param lookup_x: x coordinate\n",
    "    :param lookup_y: y coordinate\n",
    "    :return: categorical string\n",
    "    '''\n",
    "\n",
    "    # SET THIS TO TARMAC FULL TO BUILD A PROFILE AND REMOVE IN THE DATASET LIST\n",
    "\n",
    "    aoi_return = \\\n",
    "        aois_df[(aois_df['vid_type'] == vid_type) & (aois_df['cond'] == cond) & (aois_df['start_s'] <=\n",
    "                                                                                   lookup_time) &\n",
    "                (aois_df['end_s'] > lookup_time) & (aois_df['x_start'] <= lookup_x) & (aois_df['x_end'] >\n",
    "                                                                                       lookup_x)\n",
    "                & (aois_df['y_start'] <= lookup_y) & (aois_df['y_end'] > lookup_y)]['type']\n",
    "    if aoi_return.any() and aoi_return.size == 1:\n",
    "        aoi_return = aoi_return[0]\n",
    "    else:\n",
    "        aoi_return = np.nan\n",
    "    return aoi_return\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_max_length(x):\n",
    "    max_len = float('-inf')\n",
    "    for k in x.keys():\n",
    "        for l in x[k]:\n",
    "            max_len = max(len(l), max_len)\n",
    "    return max_len\n",
    "\n",
    "\n",
    "def replace_periods(sub_df, start_idx, end_idx, var):\n",
    "    '''\n",
    "    Replaces '.' or missing data, typically in saccade data with 0's\n",
    "    '''\n",
    "    tmp = sub_df[var][start_idx:end_idx]\n",
    "    tmp_mask = tmp == '.'\n",
    "    tmp[tmp_mask] = 0\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def create_tf_dataset(input_data, vid_dict, conds, model_type, aois_df, toi_in_duration, toi_out_duration, output_var,\n",
    "                      toy=False):\n",
    "    '''\n",
    "        Reads in all raw eye tracking data, and vid_dict for reference to pull out relevant regions of data for tensor.\n",
    "        :param: .npy array of asc file merge, dictionary of questions and relevant regions, conditions to filter,\n",
    "                dataframe of assessments scores by condition, vid type and participant\n",
    "                toy: forces Saccade_Duration to be 300 if participant was accurate in response (label == 1) or 0\n",
    "                otherwise\n",
    "        :return: output dataframe\n",
    "    '''\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    def calc_label(output_var, df, out_start, out_end, in_start, in_end):\n",
    "        '''\n",
    "\n",
    "        :param output_var: tuple of df number and the variable contained within that is averaged\n",
    "        :param df: sample dataframe\n",
    "        :param out: output start and end\n",
    "        :return avg_val: typically average pupil diameter value\n",
    "        '''\n",
    "        sub_df = df[output_var[0]]\n",
    "        start_idx = find_nearest(sub_df['tStart'], out_start)\n",
    "        end_idx = find_nearest(sub_df['tStart'], out_end)\n",
    "        # the following is for changes between in and out\n",
    "        #start_idx2 = find_nearest(sub_df['tStart'], in_start)\n",
    "        #end_idx2 = find_nearest(sub_df['tStart'], in_end)\n",
    "        #return np.nanmean(sub_df[output_var[1]][start_idx:end_idx]) - np.nanmean(sub_df[output_var[1]][start_idx2:end_idx2])\n",
    "        return np.nanmean(sub_df[output_var[1]][start_idx:end_idx])\n",
    "\n",
    "\n",
    "    all_data = list(input_data.keys())\n",
    "    output = collections.defaultdict(list)\n",
    "    labels = []\n",
    "    cond_list = []\n",
    "    for cond in conds:\n",
    "        for vid in vid_dict:\n",
    "            pps = [item[2] for item in all_data if cond in item and vid in item]\n",
    "            for pp in pps:\n",
    "                df = input_data[(cond, vid, pp)]\n",
    "                total_time = df[5]['tSample'][-1:].values[0]\n",
    "                total_input_output_time = toi_in_duration + toi_out_duration\n",
    "                nbins = total_time//total_input_output_time\n",
    "                for bin in range(1, nbins+1):\n",
    "                    in_start = (bin-1)*total_input_output_time  # input start\n",
    "                    in_end = in_start+toi_in_duration\n",
    "                    out_start = in_end  # output label averaged from this time\n",
    "                    out_end = bin*total_input_output_time\n",
    "                    labels.append(calc_label(output_var, df, out_start, out_end, in_start, in_end))\n",
    "                    relevant_dfs = [2, 3, 4, 5]\n",
    "                    for df_no in relevant_dfs:\n",
    "                        sub_df = df[df_no]\n",
    "                        if 'tStart' in sub_df:\n",
    "                            start_idx = find_nearest(sub_df['tStart'], in_start)\n",
    "                            end_idx = find_nearest(sub_df['tStart'], in_end)\n",
    "                            if df_no == 2:\n",
    "                                output['fixation_start'].append(sub_df['tStart'][start_idx:end_idx])\n",
    "                                output['fixation_duration'].append(sub_df['duration'][start_idx:end_idx])\n",
    "                                output['fixation_xAvg'].append(sub_df['xAvg'][start_idx:end_idx])\n",
    "                                output['fixation_yAvg'].append(sub_df['yAvg'][start_idx:end_idx])\n",
    "                                output['fixation_pupilAvg'].append(sub_df['pupilAvg'][start_idx:end_idx])\n",
    "                                # diff values\n",
    "                                output['fixation_start_diff'].append(np.diff(sub_df['tStart'][start_idx:end_idx]))\n",
    "                                output['fixation_xAvg_diff'].append(np.diff(sub_df['xAvg'][start_idx:end_idx]))\n",
    "                                output['fixation_yAvg_diff'].append(np.diff(sub_df['yAvg'][start_idx:end_idx]))\n",
    "                                tmp_aoi = sub_df[start_idx:end_idx].apply(lambda x: aoi_lookup(aois_df, x.tStart, x.xAvg, x.yAvg, vid, cond),\n",
    "                                                       axis=1)\n",
    "                                output['fixation_aoi'].append(tmp_aoi)\n",
    "                            elif df_no == 3:\n",
    "                                output['saccades_start'].append(sub_df['tStart'][start_idx:end_idx])\n",
    "                                if toy:\n",
    "                                    output['saccades_duration'].append([labels[-1] * 300] * (end_idx - start_idx))\n",
    "                                else:\n",
    "                                    output['saccades_duration'].append(sub_df['duration'][start_idx:end_idx])\n",
    "                                output['saccades_xEnd_minus_xStart'].append(\n",
    "                                    replace_periods(sub_df, start_idx, end_idx, 'xEnd').astype(float)\n",
    "                                    - replace_periods(sub_df, start_idx, end_idx, 'xStart').astype(float))\n",
    "                                output['saccades_yEnd_minus_yStart'].append(\n",
    "                                    replace_periods(sub_df, start_idx, end_idx, 'yEnd').astype(float)\n",
    "                                    - replace_periods(sub_df, start_idx, end_idx, 'yStart').astype(float))\n",
    "                                output['saccades_ampDeg'].append(sub_df['ampDeg'][start_idx:end_idx])\n",
    "                                output['saccades_vPeak'].append(sub_df['vPeak'][start_idx:end_idx])\n",
    "                                # diff values\n",
    "                                output['saccades_start_diff'].append(np.diff(sub_df['tStart'][start_idx:end_idx]))\n",
    "                            elif df_no == 4:\n",
    "                                output['blink_start'].append(sub_df['tStart'][start_idx:end_idx])\n",
    "                                output['blink_duration'].append(sub_df['duration'][start_idx:end_idx])\n",
    "                                # diff values\n",
    "                                output['blink_start_diff'].append(np.diff(sub_df['tStart'][start_idx:end_idx]))\n",
    "                    cond_list.append(cond + '-' + vid + '-' + pp + '-' + str(bin))\n",
    "    feature_list = []\n",
    "    if model_type == 'logistic':\n",
    "        tf_out = np.full((len(output['fixation_start']), len(output)), fill_value=np.nan, dtype='float32')\n",
    "        feature = 0\n",
    "        for key, values in output.items():\n",
    "            sample_no = 0\n",
    "            for l in values:\n",
    "                tf_out[sample_no, feature] = np.nanmean(l)\n",
    "                sample_no += 1\n",
    "            feature += 1\n",
    "            feature_list.append(key)\n",
    "    else:\n",
    "        max_seq_len = get_max_length(output)\n",
    "        tf_out = np.full((len(output['fixation_start']), max_seq_len, len(output)), fill_value=np.nan, dtype=object)\n",
    "        feature = 0\n",
    "        for key, values in output.items():\n",
    "            sample_no = 0\n",
    "            for l in values:\n",
    "                tf_out[sample_no, 0:len(l), feature] = l\n",
    "                sample_no += 1\n",
    "            feature += 1\n",
    "            feature_list.append(key)\n",
    "    tf_out = xr.DataArray(tf_out, coords=[('vid_type-topic-pp-block', cond_list), ('block', np.linspace(1, max_seq_len, max_seq_len, endpoint=True, dtype='int64')), ('feature', feature_list)])\n",
    "    return tf_out, labels, feature_list, cond_list\n",
    "\n",
    "# this is to cover for any differences in the long-form fixation regions, by using the sqgs instead\n",
    "vid_labels = ['tarmac', 'bicycle', 'perspective']\n",
    "conditions = ['full', 'dual', 'single']\n",
    "\n",
    "vid_dict = load_questions(vid_labels, filter_out=True)\n",
    "aois_df = load_aois()\n",
    "labels_df = merge_fixations_assessments()\n",
    "# create npy file data\n",
    "vars_list = ['dfTrial', 'dfMsg', 'dfFix', 'dfSacc', 'dfBlink', 'dfSamples']\n",
    "model_type = 'lstm'\n",
    "t0 = time.time()\n",
    "\n",
    "# load existing data\n",
    "all_eyelink_data = np.load(root_data_dir+'/all_eyelink_data.npy', allow_pickle=True)\n",
    "\n",
    "# create tensor\n",
    "toi_duration_input, toi_duration_output, output_var = 15000, 5000, (2, 'pupilAvg')\n",
    "# duration in ms, output_var in df#, varname\n",
    "tf_data, labels, feature_list, cond_list = create_tf_dataset(all_eyelink_data.item(), vid_labels, conditions,\n",
    "                                                             model_type, aois_df, toi_duration_input, toi_duration_output,\n",
    "                                                             output_var)\n",
    "\n",
    "# save models to run in lstm_model.py (as of 06/10/19)\n",
    "pickle.dump(tf_data, open( root_data_dir+'/processed_data/processed_tensor.pkl', \"wb\" ) )\n",
    "np.save(root_data_dir+'/processed_data/labels', labels)\n",
    "np.save(root_data_dir+'/processed_data/feature_list', feature_list)\n",
    "print('saved array in processed_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
